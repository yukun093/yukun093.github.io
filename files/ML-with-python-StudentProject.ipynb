{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb065a072bd51de9e21fe74783f853ce",
     "grade": false,
     "grade_id": "cell-9b77f8018c6a89bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CS-EJ3211 Machine Learning with Python \n",
    "\n",
    "## Student Project: Tissue type classification based on microarray gene expression profiles\n",
    "**submission deadline 22.03.2021 23:59 Helsinki time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a683c9528cc2bf14bf00c8025348dc6",
     "grade": false,
     "grade_id": "cell-21ce2d49b03140c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Student project instructions\n",
    "\n",
    "In order to participate in the project, you must submit a project report by 22.03.2021. The report is submitted as a Python notebook (.ipynb format), and should follow the required outline presented in this notebook.\n",
    "\n",
    "The submitted report should contain all Python code used in the project (early prototyping and \"scrapbooking\" can be excluded). The notebook should be arranged so that the reader can replicate your workflow by running the cells in the notebook in order.\n",
    "\n",
    "**General recommendations**\\\n",
    "Strive to use the notation used on this course if you use mathematical formulas or symbols. In the case that you want to use different notation, use good scientific writing principles and clearly define the meaning of your symbols.\n",
    "\n",
    "**Please comment your code.**\\\n",
    "The commenting doesn't have to be as comprehensive as it is in the exercise rounds (where it is for educational reasons), but it should give some indication of the what is happening in different sections of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eee2f66b7affd92f7926b0fefb8b258d",
     "grade": false,
     "grade_id": "cell-1f8e5bcb643b2115",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "\"A microarray is a laboratory tool used to detect the expression of thousands of genes at the same time. DNA microarrays are microscope slides that are printed with thousands of tiny spots in defined positions, with each spot containing a known DNA sequence or gene.\"\\\n",
    "text source: https://www.nature.com/scitable/definition/microarray-202/\n",
    "\n",
    "<img src=\"DNA_microarray.jpg\" width=800/>\n",
    "\n",
    "image source: https://www.genome.gov/about-genomics/fact-sheets/DNA-Microarray-Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9ee7b99a063943aacdfbf1aa0805419",
     "grade": false,
     "grade_id": "cell-5a9cb8492f55956e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The microarray data for this problem consists of normalized relative expression of certain genes measured in different tissue. There are 3000 gene probes and 2000 samples. The full dataset can be found at https://www.ebi.ac.uk/arrayexpress/ (accession number E-MTAB-62). \n",
    "\n",
    "The first columns of  'data_subset.csv' file (file located in 'coursedata' folder) contains ID's of samples (e.g. 'GSM23227.CEL') and analyses info ('RMA') and the rest - expression values for 3000 genes. \n",
    "\n",
    "Your task is to predict the type of tissue ('disease' vs 'normal') based on expression profile of samples. \n",
    "\n",
    "In addition to this task, you can solve ML problem of predicting multiple types of tissue  {'cell line', 'disease', 'neoplasm', 'normal'}. This is an optional task, which allows to earn extra points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb1060d8af2490e6b8b9bf8b8d2f395c",
     "grade": false,
     "grade_id": "cell-0fbc558ff2c5c71b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='problem'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "## Problem formulation (5 p)\n",
    "\n",
    "In contrast to the conceptual presentation of the problem in the introduction, this section formulates the problem as a machine learning problem. You should:\n",
    "\n",
    "- Define the type of your problem. Is it a regression or classification problem? Or perhaps something else?\n",
    "\n",
    "- Define the **data points** in your problem and define the **features** and **labels** of the points.\n",
    "\n",
    "- Define the **metric** that serves as the measure of quality of an ML model on your problem. For example, the mean-squared-error might be a reasonable choice for a regression problem, whereas some kind of balanced accuracy score might suit a classification problem with imbalanced classes. Note that this is not necessarily equivalent to the loss function used by your model!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TEXT HERE ###\n",
    "answer questions above\n",
    "\n",
    "- For the first question:\n",
    "At first, this is a classification problem, since we need to use dataset to classify their features. For regression,it will reflect continuous numerical values but not several clusters.\n",
    "\n",
    "- For the next question:\n",
    "Data points mean the overall data(2000 lines) in the data_subset.csv;\n",
    "features are expression values for 2999 genes;\n",
    "labels are the last column that is column label\n",
    "\n",
    "- For the next question:\n",
    "Training accuracy and validation accuracy can be used as metric when doing a classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Source Name    0          1         2         3          4         5  \\\n",
      "0       GSM23227.CEL  RMA   8.113306  8.436494  6.373835   7.985305  4.334113   \n",
      "1     1229968152.CEL  RMA  10.633046  7.286210  6.955318   9.168461  4.314263   \n",
      "2      GSM133626.CEL  RMA   8.527562  8.262509  6.725264  10.009785  4.439494   \n",
      "3       GSM47465.CEL  RMA   8.899864  7.210738  6.677196   9.336546  4.437717   \n",
      "4      GSM124909.CEL  RMA   8.703130  7.426840  6.441970   8.597625  4.348691   \n",
      "...              ...  ...        ...       ...       ...        ...       ...   \n",
      "1995    GSM21215.CEL  RMA   9.375530  5.813558  6.654986   9.104328  5.100172   \n",
      "1996    GSM29703.CEL  RMA   9.174483  6.770083  6.570502   8.571956  4.370335   \n",
      "1997    GSM29752.CEL  RMA   8.876003  6.987061  6.890687   8.464895  4.223508   \n",
      "1998    GSM29750.CEL  RMA   9.361270  7.050273  6.825901   8.558946  4.187855   \n",
      "1999  1347769995.CEL  RMA  10.801932  6.242204  6.611269   9.325318  4.796832   \n",
      "\n",
      "             6         7         8  ...      2992      2993      2994  \\\n",
      "0     6.782869  5.877186  5.293819  ...  6.180822  8.888578  8.130536   \n",
      "1     7.913541  5.829411  5.412891  ...  6.168112  7.984214  7.384627   \n",
      "2     7.315329  6.133931  5.221155  ...  6.576885  8.857609  7.133875   \n",
      "3     6.967421  5.936354  5.736722  ...  7.007417  8.150330  7.314713   \n",
      "4     6.931876  5.957056  5.304776  ...  7.020991  7.771465  7.321219   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1995  7.352827  6.010764  5.039133  ...  5.713392  8.492810  7.227065   \n",
      "1996  7.253306  5.787884  5.298322  ...  6.280876  7.726909  7.766690   \n",
      "1997  8.484922  5.889175  5.226723  ...  5.823719  8.167693  7.777821   \n",
      "1998  7.381319  5.709240  5.423488  ...  6.909454  7.775794  8.703834   \n",
      "1999  7.359541  6.008354  5.543770  ...  5.704149  8.513525  7.901915   \n",
      "\n",
      "          2995      2996      2997      2998      2999      3000      label  \n",
      "0     6.376460  7.936580  7.153437  6.624472  5.371393  6.169671  cell line  \n",
      "1     6.353065  7.521229  8.098390  7.015168  6.000483  6.829097  cell line  \n",
      "2     6.419561  7.775441  7.892436  6.843814  6.231910  7.432128  cell line  \n",
      "3     6.340884  7.610572  8.068214  7.097992  5.776570  7.005940  cell line  \n",
      "4     6.045605  7.539464  7.977012  6.076515  5.694630  6.521549  cell line  \n",
      "...        ...       ...       ...       ...       ...       ...        ...  \n",
      "1995  7.284461  7.597544  7.856710  7.483863  5.540861  7.907277     normal  \n",
      "1996  6.531365  7.545859  7.363260  6.769831  5.538058  7.245244     normal  \n",
      "1997  7.477391  7.706851  8.490099  8.495358  5.775298  7.700667     normal  \n",
      "1998  6.520055  7.276214  7.924072  7.663724  5.693761  7.199942     normal  \n",
      "1999  6.271624  7.156303  7.994740  7.468739  6.320872  7.956430     normal  \n",
      "\n",
      "[2000 rows x 3003 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_dataset = pd.read_csv(\"/coursedata/R7_StudentProject/data_subset.csv\")\n",
    "print(original_dataset.loc[0:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/coursedata/R7_StudentProject/data_subset.csv\",header=None).to_numpy()\n",
    "\n",
    "data_points = dataset[0:2001,] # shape 2000x3003 . the function of this line is to remove line title\n",
    "features = data_points[:,2:3002] # 2000x3000 remove source and name\n",
    "labels = data_points[:,3002] # 2000x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e68b4861f3b9d9edd16d352dcbc0eee7",
     "grade": false,
     "grade_id": "cell-78b62d0db4a61de4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='methods'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "# Methods\n",
    "\n",
    "## **General instructions:**\n",
    "    \n",
    "This section presents the methods used to solve the machine learning problem and walks through the process of solving the problem. This section could include:\n",
    "\n",
    "- A description of the dataset. What is the source of the dataset? How many data points does it contain? The features and labels where already presented in the previous section but can be presented once again.\n",
    "    \n",
    "- Describe why and how the data split on subsets.\n",
    "\n",
    "- A description of the pre-processing methods that you have used on your data. \n",
    "\n",
    "- A description of the model(s) you are using to solve your machine learning problem. Of what form are the predictor functions (include formula if applicable)? What is the loss function to be minimized or maximized (include formula if applicable). You should also include a short description of the hyperparameters that you tune to optimize the model. \n",
    "    \n",
    "- If you use some tools/methods for model selection and validation (e.g. cross-validation, grid search), explain the purpose of it and how it was performed.\n",
    "    \n",
    "- A description of hyperparameter tuning and model selection process. E.g. which validation methods have you used to estimate the model performance on previously unseen data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TEXT HERE ###\n",
    "answer questions above:\n",
    "- According to the general instructions, I answered questions successively with a sequence:\n",
    "For dataset, the source is from https://www.ebi.ac.uk/arrayexpress/ (accession number E-MTAB-62). It contains 2000 data. The features and labels can be described as follows.\n",
    "\n",
    "- It is because there is need to divide data into two parts which are training data and validation data. Validation data is used to test the trained model. Basically, we can use package of train_test_split such as, \n",
    "x_train, x_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "- when we have got data, there are several steps we should know:\n",
    "PCA dimensionality reduction can be used as one of the pre-processing methods to process data.\n",
    "\n",
    "- when it comes to select model to solve this problem, I think KNN(K-nearest neighbors, SVM(support vector machine), or decision tree could be used to train data, but I prefer to use SVM model. when it comes to the predictor function, there are several methods that can be used. Sigmoid function, Relu or tanh predictor function can be regarded as one of the popular linear classifiers. Sigmoid function seems a practical predictor function, and when h(x) is more than 0.5, the assigned class is 1. Instead, when h(x) is less than 0.5, the assigned class is 0.\n",
    "\\begin{equation}\n",
    "h(\\mathbf{x})=w_0 + \\mathbf{w}^{T} \\mathbf{x}\n",
    "\\end{equation}\n",
    "The loss function is shown as follows, such as MSE\n",
    "\\begin{equation}\n",
    "    \\mathcal{E(h(\\mathbf{x}))} = \\frac{1}{m} \\sum_{i=1}^m ({y}^{(i)} - \\hat{y}^{(i)})^2 = \\frac{1}{m} \\sum_{i=1}^m ({y}^{(i)} - h(\\mathbf{x}^{(i)}))^2.\n",
    "\\end{equation}\n",
    "As for tuning hyperparameters, the aim is to get a smaller prediction error in the testing dataset, namely best learning characteristics and effects. The hyperparameter should be set before training, and it is not the result after the training. There are various hyperparameters in the different models, for example learning rate which is possibly the most important one, k in K-Nearest Neighbors, c in support vector machines, the number of hidden layers in Neural Networks. Basically, learning rate could be seen as a item of trade-off between the prediction error incurred on the training error and the complexity of a predictor. A large one focus on the complexity of predictor function, and instead, a smaller one is pay attention to returning a smaller average loss.\n",
    "\n",
    "- The purpose of model selection and validation is to avoid overfitting, and select the best hyperparameter. Another function is to estimate the performance of the model and select a better one where there are several models provided to choose. K-fold method could be used to divide the dataset into training data and testing data. Searching method such as Grid search or Randomized search builds a model for every combination of hyperparameters specified and evaluates each model. Using GridSearchCV() to tune hyperparameters before .fit() and after model selection such as Lasso().\n",
    "\n",
    "- When creating a machine learning model, it should go to present with design choices as to how to define model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning. Once the parameters of model has been decided, the structure of model also is decided. Visual inspection is one of methods to do the validation process after the training model is used to train new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_points[:,2:3002] # 2000x3000\n",
    "labels = data_points[:,3002] # 2000x1, pick the first index 0, but not pick the last index 3003 though the size of label is 2000x3003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91e4642e33b50274930e1ed817b787b0",
     "grade": false,
     "grade_id": "cell-3f27e3b1d26560af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Specific instructions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17e3d1d889a4c9127df82b282a838be0",
     "grade": false,
     "grade_id": "cell-bb2bc315cf50b8f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PART 1 (mandatory, 15 p) \n",
    "\n",
    "    \n",
    "Your task is to build logistic regression and Support Vector Machine (SVM) models for solving tissue type prediction task. During this course, you have familiarized yourself with multiple ML methods from scikit-learn library, but now you will need to independently learn the specifics of how to use the SVM classifier in scikit-learn by studying the documentation and related resources. \n",
    "    \n",
    "More precisely, you need to:\n",
    "\n",
    "1. Upload the \"data_subset.csv\" file as a Pandas dataframe. The file contains gene expression data for tissues of different types. The first column contains the sample id and the second column indicates how the data was analysed (Robust Multi-array Average or RMA). The remaining columns, excluding the final one, contain the relative gene expression values. Finally, the last column contains the category (label) to which the data points belong to ('cell line', 'disease', 'neoplasm', 'normal'). \n",
    "\n",
    "\n",
    "2. In this part, you will only use data points belonging to two of the four categories in the dataset - 'disease' and 'normal'. Consequently, you should create a new data frame that only contains the data points with these labels. The new dataset should consist of 700 data points.\n",
    "\n",
    "\n",
    "3. Create numpy arrays `X` (feature matrix) and `y` (label vector) based on the data frame. The feature matrix should contain the expression data and be of shape `(700, 3000)`.\n",
    "   The label vector `y` should be of shape=(700,) and contain integer values 1 (for data points labled as \"disease\") and 0 (for data points labled as 'normal').\n",
    "   \n",
    "4. Split the data with `train_test_split` into training and test sets (with 80:20 ratio, random_state=42). Keep test set aside until final evaluation. Use training data to choose the model. \n",
    "\n",
    "5. Implement PCA (using 20 components) with logistic regression:\n",
    "\n",
    "   - Use Pipeline sklearn class to chain pre-processing steps (StandardScaler() and PCA(n_components=20, random_state=42)) and logistic regression. \n",
    "   - Use `cross_val_score class` from sklearn.model_selection to perform 5-fold cross-validation and get average F1-score (use parameters scoring='f1' and cv=5 in `cross_val_score object`).\n",
    " \n",
    "\n",
    "6. Implement PCA (using 20 components) with SVM:\n",
    "\n",
    "  - Construct Pipeline object with scaler and PCA for SVM model in a similar way as for logistic regression.\n",
    "  - Use training set for choosing parameters and hyperparameters. Specifically, perform grid search combined with cross-validation on the Pipeline object by using the `GridSearchCV` class in scikit-learn. \n",
    "  \n",
    "  The candidate parameter values for the SVM model in your grid search should be `'C': [0.01, 1, 100]` and `'gamma': [1e-04, 1e-03, 1e-02]}`, the number of folds used for cross-validation should be `cv=5`, and scoring parameter `f1`.\n",
    "  - Report F1-score of SVM model with best parameter values for `C` and `gamma`.\n",
    "  \n",
    "\n",
    "7. Choose model with best F1-score and perform final evaluation:\n",
    "\n",
    "    - Fit model (pipeline object) on the training dataset.\n",
    "    - Report the accuracy and F1-score on the training and test sets.\n",
    "    - Plot a normalized confusion matrix for the test set. \n",
    "\n",
    "Useful links:\n",
    "\n",
    "- Learn about Support Vector Machine (SVM) methods (e.g. https://scikit-learn.org/stable/modules/svm.html#support-vector-machines) and the implementation of SVM (specifically the SVC) in the scikit-learn library.\n",
    "- Pipeline example https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html\n",
    "- Metrics for evaluation https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- Function for plotting confusion matrix https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished getting data with two labels\n"
     ]
    }
   ],
   "source": [
    "### QUESTION 1 ###\n",
    "### YOUR CODE HERE ###\n",
    "### PART 1 AND 2 ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# original_dataset = pd.read_csv(\"/coursedata/R7_StudentProject/data_subset.csv\") # DataFrame\n",
    "original_dataset = pd.read_csv(\"/coursedata/R7_StudentProject/data_subset.csv\")\n",
    "df2 = original_dataset\n",
    "\n",
    "# picking up dataframe data should use dataframe.loc[][]\n",
    "for i in range(df2.shape[0]):\n",
    "    if(df2.loc[i][3002] == \"cell line\" or df2.loc[i][3002] == \"neoplasm\"):\n",
    "        df2.drop(i,axis = 0,inplace = True)\n",
    "\n",
    "print(\"finished getting data with two labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 3 ###\n",
    "df2_store = df2\n",
    "df2['label'] = df2['label'].map({'disease': 1, 'normal': 0})\n",
    "df2.drop(\"Source Name\",axis = 1,inplace = True)\n",
    "df2.drop(\"0\",axis = 1,inplace = True)\n",
    "\n",
    "# df3 = df3.values # pd.DataFrame, np->pd; df.values, pd->np\n",
    "# X = df3[:,0:3000] # 700x3000\n",
    "# y_temp = df3[:,3000] # 700x1\n",
    "# y = np.zeros(shape=(700,1),dtype=int)\n",
    "# for i in range(df3.shape[0]):\n",
    "#     if(y_temp[i] == \"disease\"):\n",
    "#         y[i] = 1\n",
    "#     elif(y_temp[i] == \"normal\"):\n",
    "#         y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "      <th>3000</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>8.841740</td>\n",
       "      <td>6.128903</td>\n",
       "      <td>6.777261</td>\n",
       "      <td>9.022795</td>\n",
       "      <td>4.408391</td>\n",
       "      <td>7.888266</td>\n",
       "      <td>6.314197</td>\n",
       "      <td>5.371741</td>\n",
       "      <td>6.574600</td>\n",
       "      <td>4.403353</td>\n",
       "      <td>...</td>\n",
       "      <td>5.571061</td>\n",
       "      <td>8.349877</td>\n",
       "      <td>7.215102</td>\n",
       "      <td>6.139312</td>\n",
       "      <td>7.480027</td>\n",
       "      <td>8.060038</td>\n",
       "      <td>7.898205</td>\n",
       "      <td>5.848507</td>\n",
       "      <td>8.264427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>8.670826</td>\n",
       "      <td>6.127531</td>\n",
       "      <td>7.180859</td>\n",
       "      <td>9.113487</td>\n",
       "      <td>4.462474</td>\n",
       "      <td>7.575946</td>\n",
       "      <td>6.024093</td>\n",
       "      <td>5.688475</td>\n",
       "      <td>6.678819</td>\n",
       "      <td>4.264804</td>\n",
       "      <td>...</td>\n",
       "      <td>5.776290</td>\n",
       "      <td>8.237783</td>\n",
       "      <td>7.575009</td>\n",
       "      <td>6.181293</td>\n",
       "      <td>7.538640</td>\n",
       "      <td>9.073399</td>\n",
       "      <td>8.696998</td>\n",
       "      <td>6.328636</td>\n",
       "      <td>8.275500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>8.809229</td>\n",
       "      <td>6.270196</td>\n",
       "      <td>6.778510</td>\n",
       "      <td>9.250695</td>\n",
       "      <td>4.396258</td>\n",
       "      <td>7.527639</td>\n",
       "      <td>6.149267</td>\n",
       "      <td>5.587484</td>\n",
       "      <td>6.401677</td>\n",
       "      <td>4.394353</td>\n",
       "      <td>...</td>\n",
       "      <td>5.937792</td>\n",
       "      <td>8.428287</td>\n",
       "      <td>7.289294</td>\n",
       "      <td>6.100881</td>\n",
       "      <td>7.403753</td>\n",
       "      <td>8.763766</td>\n",
       "      <td>8.825195</td>\n",
       "      <td>6.572378</td>\n",
       "      <td>8.584387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>9.089377</td>\n",
       "      <td>6.648818</td>\n",
       "      <td>7.085590</td>\n",
       "      <td>8.998140</td>\n",
       "      <td>4.462842</td>\n",
       "      <td>7.869689</td>\n",
       "      <td>6.060440</td>\n",
       "      <td>5.779001</td>\n",
       "      <td>6.451494</td>\n",
       "      <td>4.155387</td>\n",
       "      <td>...</td>\n",
       "      <td>6.141272</td>\n",
       "      <td>8.329656</td>\n",
       "      <td>7.586376</td>\n",
       "      <td>6.212241</td>\n",
       "      <td>7.500111</td>\n",
       "      <td>8.180384</td>\n",
       "      <td>8.109306</td>\n",
       "      <td>6.239723</td>\n",
       "      <td>8.848850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>9.239313</td>\n",
       "      <td>6.485085</td>\n",
       "      <td>7.185377</td>\n",
       "      <td>9.530301</td>\n",
       "      <td>4.617422</td>\n",
       "      <td>7.947637</td>\n",
       "      <td>6.232575</td>\n",
       "      <td>5.675885</td>\n",
       "      <td>6.871626</td>\n",
       "      <td>4.390932</td>\n",
       "      <td>...</td>\n",
       "      <td>5.131223</td>\n",
       "      <td>7.934448</td>\n",
       "      <td>7.583246</td>\n",
       "      <td>6.098047</td>\n",
       "      <td>7.537817</td>\n",
       "      <td>8.806213</td>\n",
       "      <td>8.635038</td>\n",
       "      <td>6.964470</td>\n",
       "      <td>8.547701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2         3         4         5         6         7  \\\n",
       "500  8.841740  6.128903  6.777261  9.022795  4.408391  7.888266  6.314197   \n",
       "501  8.670826  6.127531  7.180859  9.113487  4.462474  7.575946  6.024093   \n",
       "502  8.809229  6.270196  6.778510  9.250695  4.396258  7.527639  6.149267   \n",
       "503  9.089377  6.648818  7.085590  8.998140  4.462842  7.869689  6.060440   \n",
       "504  9.239313  6.485085  7.185377  9.530301  4.617422  7.947637  6.232575   \n",
       "\n",
       "            8         9        10  ...      2992      2993      2994  \\\n",
       "500  5.371741  6.574600  4.403353  ...  5.571061  8.349877  7.215102   \n",
       "501  5.688475  6.678819  4.264804  ...  5.776290  8.237783  7.575009   \n",
       "502  5.587484  6.401677  4.394353  ...  5.937792  8.428287  7.289294   \n",
       "503  5.779001  6.451494  4.155387  ...  6.141272  8.329656  7.586376   \n",
       "504  5.675885  6.871626  4.390932  ...  5.131223  7.934448  7.583246   \n",
       "\n",
       "         2995      2996      2997      2998      2999      3000  label  \n",
       "500  6.139312  7.480027  8.060038  7.898205  5.848507  8.264427      1  \n",
       "501  6.181293  7.538640  9.073399  8.696998  6.328636  8.275500      1  \n",
       "502  6.100881  7.403753  8.763766  8.825195  6.572378  8.584387      1  \n",
       "503  6.212241  7.500111  8.180384  8.109306  6.239723  8.848850      1  \n",
       "504  6.098047  7.537817  8.806213  8.635038  6.964470  8.547701      1  \n",
       "\n",
       "[5 rows x 3001 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500    1\n",
       "501    1\n",
       "502    1\n",
       "503    1\n",
       "504    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df2.drop('label',axis=1)\n",
    "X.head()\n",
    "y = df2['label']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 3000) (140, 3000) (560,) (140,)\n"
     ]
    }
   ],
   "source": [
    "### QUESTION 4 ###\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle = True, stratify = y, random_state = 42)\n",
    "y_train = y_train.ravel()\n",
    "y_val = y_val.ravel()\n",
    "print(X_train.shape,X_val.shape,y_train.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(X_train,y_train)\n",
    "\n",
    "# y_pred = lr.predict(X_val)\n",
    "# print(accuracy_score(y_val,y_pred))\n",
    "\n",
    "# y_pred_train = lr.predict(X_train)\n",
    "# print(accuracy_score(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 5 ###\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "n = 20\n",
    "np.random.seed(1)\n",
    "pca = PCA(n_components = n,random_state = 42)\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "pipe = Pipeline([('scaler',scaler), ('pca',pca), ('log_reg',log_reg)]) # create estimator with LogisticRegression\n",
    "# pipe.fit(X_train,y_train)\n",
    "# pred_y = pipe.predict(X_val)\n",
    "# accuracy = 1 - mean_squared_error(y_val,pred_y)\n",
    "accuracy = np.mean(cross_val_score(pipe, X_train, y_train, cv=5, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6250343936801497"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for best C and gamma are [0.87341772 0.83606557]\n"
     ]
    }
   ],
   "source": [
    "### QUESTION 6 ###\n",
    "### PART 1 AND PART 2 ###\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "scaler_svm = StandardScaler()\n",
    "n = 20\n",
    "np.random.seed(1)\n",
    "pca_svm = PCA(n_components=n)\n",
    "svc_svm = SVC()\n",
    "\n",
    "C = [0.01, 1, 100]\n",
    "gamma = [1e-04, 1e-03, 1e-02]\n",
    "param = {\n",
    "    'svc__C' : C,\n",
    "    'svc__gamma' : gamma\n",
    "}\n",
    "\n",
    "pipe_svm = Pipeline([('scaler',scaler_svm),('pca',pca_svm),('svc',svc_svm)]) # create estimator with SVM\n",
    "grid_search_cv = GridSearchCV(pipe_svm, param, cv=5, scoring='f1').fit(X_train,y_train)\n",
    "### PART 3 ###\n",
    "# print(grid_search_cv.best_params_) # C=100,gamma=1e-4\n",
    "C_best = [100]\n",
    "gamma_best = [1e-4]\n",
    "param_best = {\n",
    "    'svc__C':C_best,\n",
    "    'svc__gamma':gamma_best\n",
    "}\n",
    "grid_search_cv_best = GridSearchCV(pipe_svm, param_best, cv=5, scoring='f1').fit(X_train,y_train)\n",
    "pred_y_best = grid_search_cv_best.predict(X_train)\n",
    "f1_score_res = f1_score(y_train, pred_y_best, average=None)\n",
    "print(\"F1-score for best C and gamma are {}\".format(f1_score_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal : 0, disease : 1\n",
      "average training accuracy:0.8554,average testing accuracy:0.8071\n",
      "training F1-score:[0.87163233 0.83435583],testing F1-score:[0.82802548 0.7804878 ] for disease and normal tumor respectively in each dataset\n",
      "Confusion matrix, without normalization\n",
      "[[65 15]\n",
      " [12 48]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### QUESTION 7 ###\n",
    "### PART 1 AND 2 ###\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler_svm = StandardScaler()\n",
    "n = 20\n",
    "np.random.seed(1)\n",
    "pca_svm = PCA(n_components=n)\n",
    "svc_svm = SVC()\n",
    "\n",
    "C_best = [100]\n",
    "gamma_best = [1e-04]\n",
    "param_best = {\n",
    "    'svc__C' : C_best,\n",
    "    'svc__gamma' : gamma_best\n",
    "}\n",
    "\n",
    "pipe_svm = Pipeline([('scaler',scaler_svm),('pca',pca_svm),('svc',svc_svm)]) # create estimator with SVM\n",
    "grid_search_cv = GridSearchCV(pipe_svm, param, cv=5, scoring='f1').fit(X_train,y_train)\n",
    "pred_y_best_train = grid_search_cv.predict(X_train)\n",
    "tra_acc = accuracy_score(y_train, pred_y_best_train)\n",
    "f1_score_res_train = f1_score(y_train, pred_y_best_train, average=None)\n",
    "\n",
    "pred_y_best_val = grid_search_cv.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, pred_y_best_val)\n",
    "f1_score_res_val = f1_score(y_val, pred_y_best_val, average=None)\n",
    "print(\"normal : 0, disease : 1\")\n",
    "print(\"average training accuracy:{:0.4f},average testing accuracy:{:0.4f}\".format(tra_acc, val_acc))\n",
    "print(\"training F1-score:{},testing F1-score:{} for disease and normal tumor respectively in each dataset\".format(f1_score_res_train, f1_score_res_val))\n",
    "\n",
    "### PART 3 ###\n",
    "disp = plot_confusion_matrix(grid_search_cv,X_val,y_val,cmap=plt.cm.Blues,normalize=None)\n",
    "title = 'Confusion matrix, without normalization'\n",
    "disp.ax_.set_title('Confusion matrix, without normalization')\n",
    "print(title)\n",
    "print(disp.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TEXT HERE ###\n",
    "\n",
    "#### Part 1 ####\n",
    "- For this part, the pd.read_csv(\"path directory\") method is used to read the file. Actually, the data is already a form of DataFrame, so there is no need to call pd.DataFrame, otherwise, it is necessary to call this instance. \n",
    "- Here, using .copy() to store a same file to be a backup. And then use .loc to slice the dataframe, and got only one line as a file. \n",
    "- Using .drop() method to drop the title column of \"Source Name\" and \"0\", and next Using .drop() again to drop the first line. That is to say to create a empty DataFrame with correct title to store the new data later.\n",
    "\n",
    "#### Part 2 ####\n",
    "- Pick up those data with label \"disease\" and \"normal\", and write into previous empty DataFrame line by line.\n",
    "\n",
    "#### Part 3 ####\n",
    "- Using .map() to deal with replacing tasks in which disease and normal should be marked as 1 and 0 respectively.\n",
    "- Picking up the \"label\" column to be label column and Picking up the residual 3000 columns to be features columns.\n",
    "\n",
    "#### Part 4 ####\n",
    "- Using class train_test_split to split the whole dataset eithor X or y into two parts, in which 80% of X and y is training set and 20% of X and y is testing data. -  - Besides, Using revel() to transform a array for example (500,1) into (500,) to match the next operation.\n",
    "\n",
    "#### Part 5 ####\n",
    "- Using StandardScaler() to normalize data, using PCA() to decomposite data and using LogisticRegression to be an estimator.\n",
    "- using Pipeline() to coorperate them.\n",
    "- using np.mean() and cross_val_score() with parameters \"estimator, dataset, cv=5, scoring='f1'\" to get the accuracy on the training set.\n",
    "\n",
    "#### Part 6 ####\n",
    "- using class GridSearchCV to be a estimator to fit the dataset.\n",
    "- what inside is estimator Pipeline which is same with previous one, parameters with  class StandardScaler(), decomposition class PCA(n_components=n), estimator SVC() with several values in \"C\" and \"gamma\".\n",
    "- using .best_params_ in GridSearchCV to find the best parameters of C and gamma in a series of values. The best values of C and gamma are 100 and 1e-4\n",
    "- using f1_score() to get the score of F1-score\n",
    "\n",
    "####  Part 7 ####\n",
    "- using accuracy_score() to get the similarity or accuracy between prediction value and true value.\n",
    "- Finally, the F1-score of SVM model with best parameter values for C(100) and gamma(1e-4) is [0.87341772 0.83606557] which mean the accuracy of prediction of tumor with lable of disease and normal.\n",
    "- The accuracy and F1-score on the training and test sets 0.8554 and 0.8071 respectively. Besides, its F1-score on training data is [0.87163233 0.83435583] and on testing data is [0.82802548 0.7804878 ]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "321dde218900b79d9bdb42e9486396bf",
     "grade": false,
     "grade_id": "cell-d6181a52780405af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PART 2 (optional, extra 15 p max)\n",
    "\n",
    "In this part, you need to predict several tissue types ('cell line', 'disease', 'neoplasm', 'normal') based on gene expression data. Use the whole dataset and implement a SVM model for predictions.\n",
    "\n",
    "If you'd like to earn more points for the project, you can:\n",
    "    \n",
    "- Perform 3-fold cross-validation for SVM model (5 points max)  or/and perform grid search for SVM parameters `'C': [0.01, 1, 100]`, and `'gamma': [1e-04, 1e-03, 1e-02]}` (5 points max). \n",
    "- Perform 3-fold cross-validation combined with grid search (15 points max).\n",
    "\n",
    "You can either (1) implement CV or Grid Search and get 5 points max or (2) implement both, but separately, and get 10 points max, or (3) combine CV + Grid Search and get 15 points max. You need to choose only one option - e.g., you cannot do (1)+(3) and get 20 points. \n",
    "\n",
    "**NOTE!!!** In grid search for each combination of parameters report at least F1-score (you can report other metrics in addition). You should report the F1-score (average across 3 folds, if implementing CV) for each class ('cell line', 'disease', 'neoplasm', 'normal') **SEPARATELY**. This means, that you would need to perform  CV/GridSearch \"manually\", with for-loops, without using sklearn GridSearchCV and Pipeline classes. \n",
    "- During CV/GridSearch report evaluation metrics only on validation set.\n",
    "- Perform final evaluation on the test set similarly as in part 1 (report f1-score for training and test sets, and plot a confusion matrix for test set).\n",
    "\n",
    "\n",
    "Hints:\n",
    "- You can use `StratifiedKFold(n_splits=3, shuffle=True, random_state=42)` for cross-validation.\n",
    "- If using `f1_score` from sklearn.metrics, use parameter `average=None` to get f1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2 ###\n",
    "### YOUR CODE HERE ###\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "original_dataset = pd.read_csv(\"/coursedata/R7_StudentProject/data_subset.csv\")\n",
    "# dataset = pd.read_csv(\"/coursedata/R7_StudentProject/data_subset.csv\",header=None).to_numpy()\n",
    "original_dataset.head()\n",
    "df4 = original_dataset\n",
    "df4.drop(\"Source Name\", axis = 1, inplace = True)\n",
    "df4.drop(\"0\", axis = 1, inplace = True)\n",
    "df4['label'] = df4[\"label\"].map({'disease':1, 'normal':0, 'cell line':2, 'neoplasm':3})\n",
    "X_new = df4.drop('label', axis = 1)\n",
    "y_new = df4['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal : 0, disease : 1, cell_line : 2, neoplasm : 3\n",
      "gamma:0.0001,C:0.01,accuracy on labels (2,1,3,0) are 0.0000,0.0000,0.0000,0.5714 respectively\n",
      "gamma:0.001,C:0.01,accuracy on labels (2,1,3,0) are 0.0000,0.0000,0.0000,0.5714 respectively\n",
      "gamma:0.01,C:0.01,accuracy on labels (2,1,3,0) are 0.0000,0.0000,0.0000,0.5714 respectively\n",
      "gamma:0.0001,C:1,accuracy on labels (2,1,3,0) are 0.7632,0.7531,0.9816,0.9806 respectively\n",
      "gamma:0.001,C:1,accuracy on labels (2,1,3,0) are 0.9013,0.8828,1.0000,1.0000 respectively\n",
      "gamma:0.01,C:1,accuracy on labels (2,1,3,0) are 1.0000,1.0000,1.0000,1.0000 respectively\n",
      "gamma:0.0001,C:100,accuracy on labels (2,1,3,0) are 0.9984,0.9979,1.0000,1.0000 respectively\n",
      "gamma:0.001,C:100,accuracy on labels (2,1,3,0) are 1.0000,1.0000,1.0000,1.0000 respectively\n",
      "gamma:0.01,C:100,accuracy on labels (2,1,3,0) are 1.0000,1.0000,1.0000,1.0000 respectively\n",
      "best parameters of gamma and c for class 0 are 0.01 and 100 respectively\n",
      "best parameters of gamma and c for class 1 are 0.01 and 100 respectively\n",
      "best parameters of gamma and c for class 2 are 0.01 and 100 respectively\n",
      "best parameters of gamma and c for class 3 are 0.01 and 100 respectively\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_trainval,X_test,y_trainval,y_test = train_test_split(X_new, y_new, test_size=0.2, shuffle = True, stratify = y_new, random_state = 1)\n",
    "score_svm = []\n",
    "\n",
    "best_score_0 = 0\n",
    "best_score_1 = 0\n",
    "best_score_2 = 0\n",
    "best_score_3 = 0\n",
    "\n",
    "target_names = ['2', '1', '3', '0']\n",
    "# Kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "# for train_index,test_index in Kfold.split(X_trainval,y_trainval):\n",
    "#     print(train_index,test_index)\n",
    "#     X_train, X_test, y_train, y_test = X_trainval[train_index], y_trainval[train_index]\n",
    "#     x_trainval_train,x_trainval_test,y_trainval_train,y_trainval_test = X_trainval[train_index], X_trainval[test_index], y_trainval[train_index], y_trainval[test_index]\n",
    "#     print(x_trainval_train,x_trainval_test,y_trainval_train,y_trainval_test)\n",
    "\n",
    "# grid search start\n",
    "# find the highest score with c and gamma\n",
    "C = [0.01, 1, 100]\n",
    "Gamma = [1e-04, 1e-03, 1e-02]\n",
    "print('normal : 0, disease : 1, cell_line : 2, neoplasm : 3')\n",
    "for C in C:\n",
    "    for gamma in Gamma:\n",
    "        svm = SVC(gamma=gamma,C=C)\n",
    "        svm.fit(X_trainval, y_trainval)\n",
    "        pred_train = svm.predict(X_trainval)\n",
    "        f1_score_res = f1_score(y_trainval, pred_train, average = None)\n",
    "        print(\"gamma:{},C:{},accuracy on labels (2,1,3,0) are {:0.4f},{:0.4f},{:0.4f},{:0.4f} respectively\".format(gamma,C,f1_score_res[0],f1_score_res[1],f1_score_res[2],f1_score_res[3]))\n",
    "        if(f1_score_res[3] >= best_score_0):\n",
    "            best_score_0 = f1_score_res[3]\n",
    "            best_parameters_normal = {'gamma':gamma, 'c':C}\n",
    "        if(f1_score_res[1] >= best_score_1):\n",
    "            best_score_1 = f1_score_res[1]\n",
    "            best_parameters_disease = {'gamma':gamma, 'c':C}\n",
    "        if(f1_score_res[0] >= best_score_2):\n",
    "            best_score_2 = f1_score_res[0]\n",
    "            best_parameters_cell_line = {'gamma':gamma, 'c':C}\n",
    "        if(f1_score_res[2] >= best_score_3):\n",
    "            best_score_3 = f1_score_res[2]\n",
    "            best_parameters_neoplasm = {'gamma':gamma, 'c':C}\n",
    "#         print(classification_report(y_trainval, pred_train, target_names=target_names))\n",
    "\n",
    "print(\"best parameters of gamma and c for class 0 are {} and {} respectively\".format(best_parameters_normal['gamma'], best_parameters_normal['c']))\n",
    "print(\"best parameters of gamma and c for class 1 are {} and {} respectively\".format(best_parameters_disease['gamma'], best_parameters_disease['c']))\n",
    "print(\"best parameters of gamma and c for class 2 are {} and {} respectively\".format(best_parameters_cell_line['gamma'], best_parameters_cell_line['c']))\n",
    "print(\"best parameters of gamma and c for class 3 are {} and {} respectively\".format(best_parameters_neoplasm['gamma'], best_parameters_neoplasm['c']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1-score:[1. 1. 1. 1.],testing F1-score:[0.44660194 0.22857143 0.58156028 0.65843621] for normal and disease tumor respectively\n",
      "Confusion matrix, without normalization\n",
      "[[ 23   2   0  55]\n",
      " [  0   8   0  52]\n",
      " [  0   0  41  59]\n",
      " [  0   0   0 160]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best parameters of gamma and c are 0.01 and 100\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gamma = 0.01\n",
    "C = 100\n",
    "svm = SVC(gamma=gamma,C=C)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "pred_y_best_train = svm.predict(X_trainval)\n",
    "f1_score_res_train = f1_score(y_trainval, pred_y_best_train, average=None)\n",
    "\n",
    "pred_y_best_test = svm.predict(X_test)\n",
    "f1_score_res_test = f1_score(y_test, pred_y_best_test, average=None)\n",
    "print(\"training F1-score:{},testing F1-score:{} for normal and disease tumor respectively\".format(f1_score_res_train, f1_score_res_test))\n",
    "\n",
    "disp = plot_confusion_matrix(svm,X_test,y_test,cmap=plt.cm.Blues,normalize=None)\n",
    "title = 'Confusion matrix, without normalization'\n",
    "disp.ax_.set_title('Confusion matrix, without normalization')\n",
    "print(title)\n",
    "print(disp.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TEXT HERE ###\n",
    "\n",
    "#### some methods and simple explaination about codes ####\n",
    "- Basically, the process of part 2 is similar with part 1, but labels are increased to 4, which here are represented by 'disease':1, 'normal':0, 'cell line':2, 'neoplasm':3.\n",
    "- Using class svm to estimate the output label with the input of dataset. Then using f1_score() to do the accuracy estimation related to the input eithor on the training set or on the testing set.\n",
    "- for specific hyperparameters C and gamma, here using two for-loops to iterate them. Using svm to do label estimation, and then using f1_score to compute accuracy. In the second loop, also computing the best gamma and c for each class, which means four labels.\n",
    "- Picking up the best hyperparameters of C and gamma to estimate the accuracy on training set and testing set. \n",
    "- Using plot_confusion_matrix() to plot figure on testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5cccc9c3eecfa8a5a3d55be9a3d0d64",
     "grade": false,
     "grade_id": "cell-1ffb81e3924af385",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='result'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "## Results (5 p)\n",
    "\n",
    "This section presents the results of the experiments. In most problems, the central result is the estimated performance of the final model on new data with respect to the chosen performance metric. In addition, you can for example, present results for different models or consider how the hyperparameters affect the models performance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TEXT HERE ###\n",
    "Actually, there are two parts about the results of the experiments, which are part1 with two labels of normal and disease and part2 with four labels of normal, disease, cell_line and neoplasm.\n",
    "\n",
    "#### part1 result with two labels ####\n",
    "- the result of part1 is almost of accuracy about 80% eithor in training set or testing set. But all in all, the testing set has a lower accuracy which is regared as a new dataset than the training set. Here the average acuracy on training set is 0.8554, and the average estimation result on testing set is 0.8071. Specifically, the accuracy on training set with method of f1_score is [0.87163233 0.83435583] and [0.82802548 0.7804878 ], in which the sequency of estimation values is in the order of 0-normal and 1-disease.\n",
    "- the best hyperparameters of C and gamma are 100 and 1e-4 respectively.\n",
    "\n",
    "#### part2 result with four labels ####\n",
    "- the result of part2 is much better than part1 on the training set. From the view of f1_score(), the accuracy of training set is almost 100% on four labels. But on the testing set, which is 0.44660194 0.22857143 0.58156028 0.65843621 for labels 2,1,3,0 respectively.\n",
    "- for different hyperparameters, the hyperparameters would have an huge effect on training set. The best hyperparameter for example gamma:0.01 and C:100 showing better result than bad hyperparameters such as gamma:0.0001 and C:0.01. For the best parameters, the result is nearly 100%, but the result with bad parameter is less than 50% or even 0 on label 0,1,2 on some rounds. So the larger the value of gamma and C are, the better the performance on model is.\n",
    "\n",
    "#### normal : 0, disease : 1, cell_line : 2, neoplasm : 3 ####\n",
    "- gamma:0.0001,C:0.01,accuracy on labels (2,1,3,0) are 0.0000,0.0000,0.0000,0.5714 respectively\n",
    "- gamma:0.001,C:0.01,accuracy on labels (2,1,3,0) are 0.0000,0.0000,0.0000,0.5714 respectively\n",
    "- gamma:0.01,C:0.01,accuracy on labels (2,1,3,0) are 0.0000,0.0000,0.0000,0.5714 respectively\n",
    "- gamma:0.0001,C:1,accuracy on labels (2,1,3,0) are 0.7632,0.7531,0.9816,0.9806 respectively\n",
    "- gamma:0.001,C:1,accuracy on labels (2,1,3,0) are 0.9013,0.8828,1.0000,1.0000 respectively\n",
    "- gamma:0.01,C:1,accuracy on labels (2,1,3,0) are 1.0000,1.0000,1.0000,1.0000 respectively\n",
    "- gamma:0.0001,C:100,accuracy on labels (2,1,3,0) are 0.9984,0.9979,1.0000,1.0000 respectively\n",
    "- gamma:0.001,C:100,accuracy on labels (2,1,3,0) are 1.0000,1.0000,1.0000,1.0000 respectively\n",
    "- gamma:0.01,C:100,accuracy on labels (2,1,3,0) are 1.0000,1.0000,1.0000,1.0000 respectively\n",
    "- best parameters of gamma and c for class 0 are 0.01 and 100 respectively\n",
    "- best parameters of gamma and c for class 1 are 0.01 and 100 respectively\n",
    "- best parameters of gamma and c for class 2 are 0.01 and 100 respectively\n",
    "- best parameters of gamma and c for class 3 are 0.01 and 100 respectively\n",
    "\n",
    "Bacially, performance on models without pca is better than models with pca. Performance on LogisticregRegression is quite equally to on SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d25c2c5b3b09f61d6e98df9d2698814",
     "grade": false,
     "grade_id": "cell-b4e407cebef9524a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='discussion'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "\n",
    "## Discussion/Conclusions (5 p)\n",
    "\n",
    "In this section you should analyze the results on a more general level and summarize the findings of your project work. If possible, you should at least answer the following questions:\n",
    "- Do the results suggest satisfactory performance of your final model, or is there much room for improvement?\n",
    "- How do your results compare to benchmarks/ solutions of others (if such are available)?\n",
    "- Are you aware of some methodological shortcomings in the project?\n",
    "- Do you have ideas for how to improve the performance (e.g. using more training data, using more features for the data points, using different class of predictor functions (hypothesis space) ?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TEXT HERE ###\n",
    "\n",
    "there are several points that should be noted which are corresponding to the questions above.\n",
    "1. yes, there is much room to improve my model, because the performance on the testing set behaves really unsatisfied, but the hyperparameters have been reached the best ones. Besides, one point that should be mentioned is that when i used LogisticRegression to fit model and do estimation, the result is better than using Pipeline with StandardScaler(), PCA() and LogisticRegression(). This is the first point i didn't fully understand.\n",
    "2. I have once read several examples through others' blogs, and someone's results of their problems is similar to mine, especially when using pca to do estimation or results of confusion metrics. I think the result is also really depandent on the dataform itself.\n",
    "3. Actually, i confronted methological shortcomings from time to time, such as unfamiliar with slicing in dataframe, dividing labels with numbers which could use map() to taking place of manually signing. I don't know whether it is the best method with Pipeline() to train the model with thousands of features since the features are not easily to classify. Compared with machine learning, there is another way using deep learning to train the model, and possibly the accuracy could be higher.\n",
    "4. i think using more training data is a potential method to increase the accuracy when especially the number of features is more than the number of samples. Besides, algorithm tuning seems a method to modify the hyperparameters in training. Or using several algotithms to matching data, because some algorithms are better suited to a particular type of data sets than others. Hence, it is necessary to apply all relevant models and check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
